{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Subjectivity Sentiment in News Articles\n",
    "\n",
    "This notebook outlines my process for getting articles from various news sources, cleaning up the text, and calculating the subjectivity of each article using TextBlob.  Some things to keep in mind:\n",
    "    - When calculating the sentiments, Textblob is looking at the adjectives use in the sentence and uses the scores from WordNet to determine subjectivity.\n",
    "    - The subjectivity for each article is determined by taking the mean of the sentiments for each sentence in an article.\n",
    "    - The final ranking seems to be more about percentage of reporting versus analysis/editorial than a measure of actual objectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean, Tokenize and Calculate Objectivity Sentiment for Text\n",
    "\n",
    "The text that we get from scraping the articles has some unwanted characters like unicode, punctuation and linebreaks.  We still want to keep the periods so that we can tokenize our sentences properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    removed = text.replace(\"\\n\\n\", \" \")\n",
    "    clean = filter(lambda x: x in string.printable, removed)\n",
    "    return \"\".join(l for l in clean if l not in string.punctuation.replace(\".\",\"\"))\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    return [sent for sent in sents if len(sent.split()) >= 5]\n",
    "\n",
    "def objectivity_sentiment(text):\n",
    "    sentiments = []\n",
    "    for sent in text:\n",
    "        sentiments.append(TextBlob(sent).sentiment[1])\n",
    "    if len(sentiments) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return sum(sentiments) / float(len(sentiments))\n",
    "\n",
    "def process_text(text):\n",
    "    clean = clean_text(text)\n",
    "    tokens = tokenize_text(clean)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Articles for a News Source\n",
    "\n",
    "Using the python library, Newspaper, we can build a list of articles and then iterate through, download the article, parse it and extract the information we want into a dictionary and then the list of dictionaries for the given news site is dumped to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(name, url):\n",
    "    paper = newspaper.build(url, memoize_articles=False)\n",
    "    article_list = []\n",
    "    for article in paper.articles:\n",
    "        art = {}\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = process_text(article.text)\n",
    "        if text:\n",
    "            art[\"title\"] = article.title\n",
    "            art[\"authors\"] = article.authors\n",
    "            art[\"text\"] = text\n",
    "            art[\"sentiment\"] = objectivity_sentiment(text)\n",
    "            article_list.append(art)\n",
    "            \n",
    "    filename = name + '_articles.json'\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(article_list, fp, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are reading our list of news sources we want to scrape, the list is in a CSV format where the first field is the name of the news outlet and the second field is the url of their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"news_sources.csv\")\n",
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = news_df['outlet'].tolist()\n",
    "urls = news_df['url'].tolist()\n",
    "for n, u in zip(names,urls):\n",
    "    get_articles(n,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('temp/*.json')\n",
    "print len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up Articles\n",
    "I decided to try and focus on longer articles as after reading several it seemed that a lot of the short articles were just summaries of news stories or promotions for movies which is not what I am interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_short(data):\n",
    "    d_list = []\n",
    "    for d in data:\n",
    "        if len(d[\"text\"]) > 10:\n",
    "            d_list.append(d)\n",
    "    return d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fname in files:\n",
    "    with open(fname) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        d_list = filter_short(data)\n",
    "    with open(fname, 'w') as fp:\n",
    "        json.dump(d_list, fp, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Ranking\n",
    "The final goal of this part of the project was to rank the various outlets in ascending order starting with the lowest subjectivity score.  To do this I calculated the average of all of the sentiments of the documents for each of the news outlets and then sorted them.  It is interesting to note that RT comes out on top as they are reletively infamous for being biased, however the limitation of my model doesn't account for bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_sentiments(data):\n",
    "    sent = 0\n",
    "    for d in data:\n",
    "        sent += d[\"sentiment\"]\n",
    "    return sent/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, temp/RT_articles.json\n",
      "2, temp/Reuters_articles.json\n",
      "3, temp/CBSNews_articles.json\n",
      "4, temp/WashingtonPost_articles.json\n",
      "5, temp/ABC_articles.json\n",
      "6, temp/Bloomberg_articles.json\n",
      "7, temp/fox_articles.json\n",
      "8, temp/NBC_articles.json\n",
      "9, temp/USAToday_articles.json\n",
      "10, temp/CNN_articles.json\n",
      "11, temp/Economist_articles.json\n",
      "12, temp/NYPost_articles.json\n",
      "13, temp/BBC_articles.json\n",
      "14, temp/Guardian_articles.json\n",
      "15, temp/AlJazeera_articles.json\n",
      "16, temp/NPR_articles.json\n",
      "17, temp/NYT_articles.json\n",
      "18, temp/ArsTechnica_articles.json\n",
      "19, temp/Buzzfeed_articles.json\n",
      "20, temp/HuffPost_articles.json\n",
      "21, temp/Wired_articles.json\n",
      "22, temp/Verge_articles.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_list = []\n",
    "for fname in files:\n",
    "    with open(fname) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        sent = sum_sentiments(data)\n",
    "        sent_list.append((fname, sent))\n",
    "sents = sorted(sent_list, key=lambda x: x[1])\n",
    "ranking = \"\"\n",
    "for i, x in enumerate(sents):\n",
    "    ranking += str(i+1)+', ' + x[0] + '\\n'\n",
    "print ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Corpus\n",
    "I wanted to do some topic modeling to allow me to compare news outlets on a topic by topic basis to see which topics are potentially contraversial and if certain outlets were more interested in certain topics than others.  In order to do this I needed to build a corpus of documents from all of the outlets and then I dumped the resulting list of documents to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for fname in files:\n",
    "    with open(fname) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        for d in data:\n",
    "            corpus.append(' '.join(d['text']))\n",
    "            \n",
    "output = open('corpus.pkl', 'wb')\n",
    "pickle.dump(corpus, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5036\n"
     ]
    }
   ],
   "source": [
    "pkl_file = open('corpus.pkl', 'rb')\n",
    "corpus = pickle.load(pkl_file)\n",
    "print len(corpus)\n",
    "pkl_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
